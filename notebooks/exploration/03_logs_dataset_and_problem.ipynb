{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logs Dataset: Preprocessing and Problem Formulation\n",
        "\n",
        "This notebook demonstrates dataset description/preprocessing and problem formulation for a lean log intelligence pipeline.\n",
        "yo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coding style: clear imports and configuration\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (11, 5)\n",
        "DATA = Path('../../data/raw/synthetic')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic logs if missing\n",
        "if not (DATA / 'logs.csv').exists():\n",
        "    import subprocess, sys\n",
        "    subprocess.check_call([sys.executable, '../../scripts/generate_synthetic_logs.py', '--out', str(DATA / 'logs.csv'), '--n', '5000'])\n",
        "\n",
        "logs = pd.read_csv(DATA / 'logs.csv', parse_dates=['timestamp'])\n",
        "logs.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess\n",
        "logs = logs.copy()\n",
        "logs['timestamp'] = logs['timestamp'].dt.tz_localize('UTC')\n",
        "severity_map = {'INFO':0, 'WARN':1, 'ERROR':2}\n",
        "logs['severity_num'] = logs['severity'].map(severity_map)\n",
        "logs = logs.drop_duplicates(subset=['timestamp','host','message'])\n",
        "logs['len'] = logs['message'].str.len()\n",
        "logs.describe(include='all')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stratified split\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(logs, test_size=0.2, stratify=logs['category'], random_state=42)\n",
        "val_df, test_df = train_test_split(test_df, test_size=0.5, stratify=test_df['category'], random_state=42)\n",
        "len(train_df), len(val_df), len(test_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Formulation\n",
        "- Goal: classify log chunks into incident categories; extract root cause / remediation text.\n",
        "- Constraints: p95 latency < 1s per 1k lines; cost budget <$0.50 per 100k lines.\n",
        "- Metrics: accuracy, macro‑F1, per‑class PR/F1, calibration (ECE/Brier).\n",
        "- Baseline: keyword rules; Model: compact GPT‑4‑family with streaming.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ML Baseline: TF‑IDF + Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1,2), min_df=2, max_features=50000, sublinear_tf=True)),\n",
        "    ('clf', LogisticRegression(max_iter=200, class_weight='balanced', n_jobs=None, solver='liblinear')),\n",
        "])\n",
        "\n",
        "pipeline.fit(train_df['message'], train_df['category'])\n",
        "probs = pipeline.predict_proba(val_df['message'])\n",
        "preds = pipeline.classes_[probs.argmax(axis=1)]\n",
        "print(classification_report(val_df['category'], preds))\n",
        "confusion_matrix(val_df['category'], preds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calibration: ECE and Brier Score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import brier_score_loss\n",
        "\n",
        "# Multiclass Brier: average over one-vs-all\n",
        "classes = list(pipeline.classes_)\n",
        "y_true = val_df['category'].values\n",
        "\n",
        "brier_list = []\n",
        "for k, c in enumerate(classes):\n",
        "    y_bin = (y_true == c).astype(int)\n",
        "    p = probs[:, k]\n",
        "    brier_list.append(brier_score_loss(y_bin, p))\n",
        "\n",
        "brier = float(np.mean(brier_list))\n",
        "print('Brier (multiclass mean):', round(brier, 4))\n",
        "\n",
        "# Expected Calibration Error (ECE)\n",
        "def expected_calibration_error(y_true, probas, bins=10):\n",
        "    # y_true: labels; probas: [N, K]\n",
        "    confidences = probas.max(axis=1)\n",
        "    predictions = probas.argmax(axis=1)\n",
        "    correct = (classes_np[predictions] == y_true)\n",
        "    bin_bounds = np.linspace(0.0, 1.0, bins + 1)\n",
        "    ece = 0.0\n",
        "    for i in range(bins):\n",
        "        lo, hi = bin_bounds[i], bin_bounds[i+1]\n",
        "        mask = (confidences > lo) & (confidences <= hi)\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "        acc = correct[mask].mean()\n",
        "        conf = confidences[mask].mean()\n",
        "        ece += (mask.mean()) * abs(acc - conf)\n",
        "    return float(ece)\n",
        "\n",
        "classes_np = np.array(classes)\n",
        "ece = expected_calibration_error(y_true, probs, bins=15)\n",
        "print('ECE:', round(ece, 4))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
