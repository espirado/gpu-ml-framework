{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2835b6b3-1efb-4a11-ba6d-a4db7d4bbd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented Sentences:\n",
      "1. Away from the city lights we were able to see the Milky Way.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andrewespira/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Task 1: Sentence Tokenization\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Download required resource\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample paragraph\n",
    "text = \"Away from the city lights we were able to see the Milky Way.\"\n",
    "\n",
    "# Split into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Show the segmented sentences\n",
    "print(\"Segmented Sentences:\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sentence}\")\n",
    "\n",
    "# Explanation:\n",
    "# This code downloads the required NLTK resource ('punkt') and uses sent_tokenize \n",
    "# to split a paragraph into individual sentences.\n",
    "# It then prints each sentence in a numbered list format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b604927-f3bb-43f6-ae8d-4facc663ee72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original     : ['The', 'runners', 'were', 'running', 'and', 'caring', 'about', 'their', 'connections', '.']\n",
      "Stemmed      : ['the', 'runner', 'were', 'run', 'and', 'care', 'about', 'their', 'connect', '.']\n",
      "Lemmatized   : ['The', 'runner', 'were', 'running', 'and', 'caring', 'about', 'their', 'connection', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andrewespira/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andrewespira/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/andrewespira/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Taskk2 \n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The runners were running and caring about their connections.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Apply stemming\n",
    "stemmed = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Print results\n",
    "print(\"Original     :\", tokens)\n",
    "print(\"Stemmed      :\", stemmed)\n",
    "print(\"Lemmatized   :\", lemmatized)\n",
    "\n",
    "#This code tokenizes a sentence into words, then applies stemming and lemmatization to show how each word is reduced to its root or base form.\n",
    "#It prints the original tokens, their stemmed versions, and their lemmatized versions for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86663d63-ecb5-4798-9c92-aee57596d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['Away', 'from', 'the', 'city', 'lights', ',', 'we', 'were', 'able', 'to', 'see', 'the', 'Milky', 'Way']\n",
      "Filtered tokens (no stop words): ['Away', 'city', 'lights', ',', 'able', 'see', 'Milky', 'Way']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andrewespira/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andrewespira/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Task 3\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords (only needed once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample sentence\n",
    "text = \"Away from the city lights, we were able to see the Milky Way\"\n",
    "\n",
    "# Step 1: Tokenize the sentence\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Step 2: Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Print results\n",
    "print(\"Original tokens:\", tokens)\n",
    "print(\"Filtered tokens (no stop words):\", filtered_tokens)\n",
    "#This code tokenizes a sentence into individual words and then removes common English stopwords (like \"the\", \"and\", \"from\").\n",
    "#It prints both the original tokens and the filtered list containing only meaningful words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8ccb2ca-1da0-4feb-bc72-8170d4a9038b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original words: ['we', 'were', 'able', 'to', 'see', 'the', 'Milky', 'Way']\n",
      "Stemmed words : ['we', 'were', 'abl', 'to', 'see', 'the', 'milki', 'way']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andrewespira/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary data if running for the first time\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Create stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"we were able to see the Milky Way\"\n",
    "\n",
    "# Tokenize sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Stem each word\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Show results\n",
    "print(\"Original words:\", tokens)\n",
    "print(\"Stemmed words :\", stemmed_words)\n",
    "#This code tokenizes a sentence into words and applies Porter stemming to reduce each word to its root form.\n",
    "#It then prints both the original tokens and their stemmed versions for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6310eee9-a0c2-4a11-ac58-912eaf827e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original     : ['we', 'were', 'able', 'to', 'see', 'the', 'Milky', 'Way']\n",
      "Stemmed      : ['we', 'were', 'abl', 'to', 'see', 'the', 'milki', 'way']\n",
      "Lemmatized   : ['we', 'were', 'able', 'to', 'see', 'the', 'Milky', 'Way']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andrewespira/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/andrewespira/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/andrewespira/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Task 4\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"we were able to see the Milky Way\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Apply stemming\n",
    "stemmed = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# Apply lemmatization\n",
    "lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Print results\n",
    "print(\"Original     :\", tokens)\n",
    "print(\"Stemmed      :\", stemmed)\n",
    "print(\"Lemmatized   :\", lemmatized)\n",
    "#This code tokenizes a sentence into words, then applies stemming and lemmatization to show how each word is reduced to its root form.\n",
    "#It prints the original tokens, their stemmed versions, and their lemmatized versions side by side for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7386437b-32c8-49d3-b1fb-75d41afe7fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/andrewespira/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/andrewespira/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging Results:\n",
      "Wow        -> NN\n",
      "!          -> .\n",
      "The        -> DT\n",
      "excited    -> JJ\n",
      "dog        -> NN\n",
      "quickly    -> RB\n",
      "ran        -> VBD\n",
      "across     -> IN\n",
      "the        -> DT\n",
      "yard       -> NN\n",
      "because    -> IN\n",
      "it         -> PRP\n",
      "saw        -> VBD\n",
      "another    -> DT\n",
      "cat        -> NN\n",
      ".          -> .\n"
     ]
    }
   ],
   "source": [
    "# Task: POS Tagging with Correct Resource\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "# Download necessary data (only the first time)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')  # ✅ Correct resource for English POS tagging\n",
    "\n",
    "# Sample sentence\n",
    "sentence = \"Wow! The excited dog quickly ran across the yard because it saw another cat.\"\n",
    "\n",
    "# Step 1: Tokenize the sentence\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Step 2: POS tagging\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "# Output results\n",
    "print(\"POS Tagging Results:\")\n",
    "for word, tag in tagged:\n",
    "    print(f\"{word:10} -> {tag}\")\n",
    "\n",
    "# Explanation:\n",
    "# This code tokenizes a sentence into words and assigns part-of-speech (POS) tags \n",
    "# (like noun, verb, adjective) to each token using the updated 'averaged_perceptron_tagger_eng'.\n",
    "# It then prints each word along with its corresponding POS tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b1f13-405a-49c4-88b4-9d8f47d930f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
